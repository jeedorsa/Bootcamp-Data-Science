{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8fa137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br/>\n",
    "<img src=\"images/utfsm.png\" alt=\"\" width=\"130px\" align=\"left\"/>\n",
    "<img src=\"images/utfsm.png\" alt=\"\" width=\"130px\" align=\"right\"/>\n",
    "<div align=\"center\">\n",
    "<h1>Introducción a Redes Neuronales y DeepLearning</h1>\n",
    "<br/><br/>\n",
    "Dr. Nicolás Gálvez Ramírez<br/>\n",
    "Dr. Patricio Olivares Roncagliolo<br/><br/>\n",
    "Ingeniería Civil Telemática<br/>\n",
    "Departamento de Eléctronica<br/>\n",
    "Universidad Técnica Federico Santa María\n",
    "</div>\n",
    "<br>\n",
    "Fuentes: \n",
    "<br>\n",
    "\"Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow\"\n",
    "<br>\n",
    "\"Standford - CS229 Notes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40308df",
   "metadata": {},
   "source": [
    "# ¿Qué es Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9091ba",
   "metadata": {},
   "source": [
    "- Machine Learning (de ahora en adelante, **ML**), se define como el conjunto de técnicas a través de los cuales un computador puede aprender de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92baa4f3",
   "metadata": {},
   "source": [
    "<img src=\"images/diagrama.png\" alt=\"\" width=\"800px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11ee1e",
   "metadata": {},
   "source": [
    "# ¿Por qué utilizar Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f125d2",
   "metadata": {},
   "source": [
    "### Ej. Creación de un filtro de spam para correos electrónicos\n",
    "\n",
    "- Aproximación de ML\n",
    "\n",
    "<img src=\"images/withML.png\" alt=\"\" width=\"500px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6bc06",
   "metadata": {},
   "source": [
    "### Ej. Creación de un filtro de spam para correos electrónicos\n",
    "\n",
    "- Aproximación tradicional\n",
    "\n",
    "<img src=\"images/withoutML.png\" alt=\"\" width=\"500px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffce62",
   "metadata": {},
   "source": [
    "### Ej. Creación de un filtro de spam para correos electrónicos\n",
    "\n",
    "- Aproximación de ML\n",
    "\n",
    "<img src=\"images/withML.png\" alt=\"\" width=\"500px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a45b6e",
   "metadata": {},
   "source": [
    "# Bibliotecas para ML python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591270d",
   "metadata": {},
   "source": [
    "<img src=\"images/bibliotecas.png\" alt=\"\" width=\"800px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c87425",
   "metadata": {},
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83acbc5",
   "metadata": {},
   "source": [
    "<img src=\"images/scikit-learn-logo-notext.png\" alt=\"\" width=\"200px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a40f9",
   "metadata": {},
   "source": [
    "- Biblioteca para ML de código libre\n",
    "- Biblioteca para Python\n",
    "- Soporta múltiples algoritmos tanto supervisados como no supervisados\n",
    "-  Si está trabajando en su entorno local, previo a su importación debe ser **instalada** \n",
    "    - ```pip install scikit-learn``` si usa ```pip```\n",
    "    - ```conda install scikit-learn``` si usa ```conda```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3629cc",
   "metadata": {},
   "source": [
    "# Scikit-Learn: Utilización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa683e62",
   "metadata": {},
   "source": [
    "Todo los objetos Scikit-Learn son **consistentes** entre sí\n",
    "* **Estimadores:** \n",
    "    - Método encargado de la **estimación** de los parámetros asociados al algoritmo. \n",
    "    - Objetos Scikit-Learn incluyen el método *fit()*, cuya entrada es el set de datos utilizados para el cálculo de los parámetros.\n",
    "* **Transformadores:** \n",
    "    - Método encargado de la **transformación** de datos en base a los parámetros calculados por método *fit()*. \n",
    "    - Objetos Scikit-Learn incluyen el método *transform()*, que recibe con los datos a transformar.\n",
    "    - Método *fit_transform()* permite realizar ambos pasos con un solo método.\n",
    "* **Predictores:** \n",
    "    - Método encargado de hacer **predicciones** en base a datos de entrada.\n",
    "    - Objetos Scikit-Learn incluyen el método *predict()*, para realizar predicciones en base a parámetros de entrada.\n",
    "        \n",
    "Cuáles de estos métodos están presentes, dependerá del tipo algoritmo a utilizar.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7fd14",
   "metadata": {},
   "source": [
    "# División en set de training y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f323b3",
   "metadata": {},
   "source": [
    "- Los modelos de Machine Learning intentan aprender patrones en los datos que le permitan hacer predicciones.\n",
    "- Al momento de evaluar un modelo, se desea estimar qué tan bien es capaz de **generalizar** a partir de los datos usados en el proceso de aprendizaje. (¿Puedo hacer predicciones sobre nuevos datos a partir de lo ya aprendido?)\n",
    "- Para ello, siempre es necesario dividir nuestro data set en dos grupos:\n",
    "    * **Training set**: Es el subconjunto de datos que será utilizado para entrenar nuestro modelo.\n",
    "    * **Test set**: Es el subconjunto de datos que será utilizado para evaluar la generalización de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e79ae-3710-4ba8-97f1-314d76da39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de división training/test con scikit learn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carga de dataset dentro de scikit learn\n",
    "# Dataset de cáncer de mamas\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de543514",
   "metadata": {},
   "source": [
    "# Tipos de errores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644cad5d",
   "metadata": {},
   "source": [
    "Para determinar el correcto funcionamiento de un algoritmo de clasificación, podemos preguntarnos\n",
    "\n",
    "- *¿Cuántas veces acertamos la clasificación de una clase y cuántas veces no?*\n",
    "\n",
    "Los errores que puede cometer nuestro clasificador son de dos tipos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318ef37",
   "metadata": {},
   "source": [
    "## Errores de Tipo 1: Falso Positivo\n",
    "\n",
    "Clasificamos un elemento como *perteneciente* (positivo) a la clase **incorrectamente** (falso)\n",
    "\n",
    "<img src=\"images/un5.jpeg\" alt=\"\" width=\"300px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1771b",
   "metadata": {},
   "source": [
    "## Errores de Tipo 2: Falso Negativo\n",
    "\n",
    "Clasificamos un elemento como *no perteneciente* (negativo) a la clase **incorrectamente** (falso)\n",
    "\n",
    "<img src=\"images/noUn5.jpeg\" alt=\"\" width=\"300px\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee820865",
   "metadata": {},
   "source": [
    "# Matriz de confusión\n",
    "\n",
    "Permite contar la cantidad de veces que instancias de la clase A son clasificadas como clase B\n",
    "\n",
    "<img src=\"images/matrizConfusion.png\" alt=\"\" width=\"700px\" align=\"center\"/>\n",
    "Fuente: \"Hands-on Machine Learning with Scikit-Learn, Keras & Tensorflow\", O'Reilly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0c070",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2205453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Primera vez que se importa la biblioteca de aprendizaje automático scikit-learn\n",
    "# No te preocupes si esto parece mucho código\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ten en cuenta que es probable que el nombre del archivo sea diferente en tu computadora (especialmente más adelante en el curso)\n",
    "filename = 'data/titanic.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma la columna Sexo en una forma que no sea de texto\n",
    "# Elijo cuatro funciones, podrías haber elegido otras\n",
    "feature_cols = ['Pclass', 'Parch', 'Age', 'Sex']\n",
    "# Convierta la columna Sexo en algo que pueda alimentar a un modelo\n",
    "# Tiene\n",
    "df['Sex'] = df.Sex.map({'male': 0, \n",
    "                        'female': 1})\n",
    "# Eliminar filas donde la edad es nan del conjunto de datos\n",
    "df = df.loc[~df['Age'].isnull(), :]\n",
    "# Crear matriz de características y vector objetivo\n",
    "X = df.loc[:, feature_cols]\n",
    "y = df['Survived']\n",
    "# Divida los datos en conjuntos de prueba y entrenamiento\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 0)\n",
    "# Estandarizar datos\n",
    "scaler = StandardScaler()\n",
    "# Ajusta solo en el conjunto de entrenamiento\n",
    "scaler.fit(X_train)\n",
    "# Aplicar transformación tanto al conjunto de entrenamiento como al conjunto de prueba\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Crea una instancia de un modelo\n",
    "logreg = LogisticRegression()\n",
    "# Entrena el modelo con los datos\n",
    "logreg.fit(X_train, y_train);\n",
    "# predicciones de clase (no probabilidades predichas)\n",
    "predictions = logreg.predict(X_test)\n",
    "print(predictions)\n",
    "print(y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf9443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valores reales: y_test.values\n",
    "# predicciones: predicciones del modelo\n",
    "print(\"Total de elementos:\", len(y_test.values))\n",
    "cm = metrics.confusion_matrix(y_test.values, predictions)\n",
    "print(cm)\n",
    "print(cm.ravel())\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49acde95",
   "metadata": {},
   "source": [
    "# Medidas para clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c15988",
   "metadata": {},
   "source": [
    "## Accuracy (exactitud)\n",
    "\n",
    "- Fracción de casos correctamente clasificados sobre casos totales (¿Cuántas veces acerté?)\n",
    "\n",
    "$$ \\text{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "- En nuestra matriz de confusión anterior, sería\n",
    "\n",
    "$$\\text{accuracy} = \\frac{87 + 59}{87 + 16 + 17 + 59} = 0.8156 = 81.56\\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = logreg.score(X_test, y_test) # Esta línea, para este modelo, calcula el accuracy\n",
    "print(\"Score obtenido por Regresor\", score)\n",
    "manual_score = (tp + tn)/(tp + tn + fp + fn)\n",
    "print(\"Score obtenido manualmente\", manual_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87819da7",
   "metadata": {},
   "source": [
    "El accuracy es una excelente medida, pero presenta algunos problemas. Veamos un ejemplo para un \"clasificador de 5s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ej. Modelo de clasificación de 5s\n",
    "import numpy as np\n",
    "# Datos de entrenamiento\n",
    "datos = [1,2,1,3,4,2,5,3,4,6,5,4,2,7,9,8,9,6,4,5]\n",
    "# Valor a predecir. 1 representa la existencia de un 5 y 0 la ausencia del mismo.\n",
    "valores_reales = [0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "# Crearemos el modelo \"Nada es 5\". Este modelo decide que ningún número es 5\n",
    "# Emularemos las predicciones de este modelo que clasifica todo como\n",
    "# \"No 5\" en la siguiente variable\n",
    "predicciones = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] # Crea una lista de 0s\n",
    "print(predicciones)\n",
    "print(valores_reales)\n",
    "# Nuestro modelo clasifica todo como NO 5\n",
    "tn=17 # 17 son los números que no eran 5, por lo tanto nuestro modelo los clasifico bien\n",
    "tp=0 # Como no clasificamos ningún 5, los TP son\n",
    "fn=3 # Los 5s que clasificamos incorrectamente, están acá\n",
    "fp=0 # No hicimos ninguna clasificación FP.\n",
    "\n",
    "# Ejecuta esta celda y ve qué accuracy obtiene este modelo!\n",
    "accuracy = (tn+tp)/(tn+tp+fn+fp)\n",
    "print(accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5949b7",
   "metadata": {},
   "source": [
    "A pesar de haber creado un mal modelo, obtuvimos un alto accuracy. Esto se debe a que el accuracy hace un importante supuesto:Los datos de las clases positiva y negativa deben ser **COMPARABLES** en cantidad, lo cual no siempre ocurre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafb1b2",
   "metadata": {},
   "source": [
    "## Sensitivity/Recall/True Positive Rate (sensibilidad)\n",
    "\n",
    "- Fracción de casos **pertenecientes** a una clase correctamente clasificados (¿Cuántas veces acerté al clasificar la clase X?)\n",
    "\n",
    "$$ \\text{sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\text{Positivos correctamente clasificados}}{\\text{Positivos reales totales}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, calculemos la sensibilidad de nuestro modelo de\n",
    "# No 5s\n",
    "sensitivity = tp/(tp+fn)\n",
    "print(sensitivity*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3868d",
   "metadata": {},
   "source": [
    "## Specificity/Selectivity/True Negative Rate (especificidad)\n",
    "\n",
    "- Fracción de casos **no pertenecientes** a una clase correctamente clasificados (¿Cuántas veces acerté al clasificar casos fuera de la clase X?)\n",
    "\n",
    "$$ \\text{specificity} = \\frac{TN}{TN + FP} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2deb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculemos ahora la especificidad de nuestro modelo\n",
    "# No 5s\n",
    "specificity = (tn)/(tn + fp)\n",
    "print(specificity*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b636e",
   "metadata": {},
   "source": [
    "## Precision/Positive predictions (precisión)\n",
    "\n",
    "- Fracción de casos clasificados como **positivos** que fueron correctamente clasificados (¿Cuántos casos clasificados como X fueron acertados?)\n",
    "\n",
    "$$ \\text{precision} = \\frac{TP}{TP + FP} = \\frac{\\text{Casos correctamente clasificados}}{\\text{Casos totales clasificados como Positivos}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29250815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculemos la precision de nuestro modelo\n",
    "# No 5s\n",
    "#precision = tp/(tp+fp) # Como no hay ningún caso clasificado como positivo, denominador cero\n",
    "#print(precision*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d349ab6",
   "metadata": {},
   "source": [
    "# Redes neuronales (Neural Networks - NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a7e4c",
   "metadata": {},
   "source": [
    "- Las Redes Neuronales son un conjunto de modelos de Machine Learning.\n",
    "- Estas se inspiran en el funcionamiento neurológico del cerebro humano.\n",
    "- Las redes neuronales han sido utilizadas en múltiples y diferentes áreas de aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ee6d1",
   "metadata": {},
   "source": [
    "### Ejemplos de aplicaciones Deep Learning\n",
    "\n",
    "- [DeepMind](https://www.youtube.com/watch?v=V1eYniJ0Rnk)\n",
    "- [Gaugan2](http://gaugan.org/gaugan2/?text_input=ocean+waves+in+a+beach)\n",
    "- [YoloV3](https://www.youtube.com/watch?v=MPU2HistivI)\n",
    "- [QuickDraw](https://quickdraw.withgoogle.com/#)\n",
    "- [Rock-Paper-Scisors](https://tenso.rs/demos/rock-paper-scissors/)\n",
    "- [ChatGPT](https://chat.openai.com/)\n",
    "- Y una laaaarga [lista](https://www.mygreatlearning.com/blog/deep-learning-applications/#deepdreaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee2e92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78436bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es la red neuronal más sencilla cuya base es la TLU (Threshold Logic Unit)\n",
    "- TLU es una neurona artificial que opera de la siguiente manera\n",
    "    - Recibe una o varias entradas numéricas (datos de entrenamiento) $\\mathbf{x}=(x_1, x_2, ..., x_n)$\n",
    "    - Cada entrada es ponderada por un peso $\\mathbf{w}=(w_1x_1, w_2x_2, ..., w_nx_n)$ \n",
    "    - Se calcula la suma ponderada de las entradas $z=w_1x_1 + w_2x_2 + ... +w_nx_n = \\mathbf{x}^T \\mathbf{w}$\n",
    "    - Al resultado de la suma ponderada se le aplica una función llamada función de activación $h_{\\mathbf{w}}(\\mathbf{x})$. El resultado corresponde a la predicción $\\hat{y}$.\n",
    "    \n",
    "$$z=x_0 + w_1x_1 + w_2x_2 + ... +w_nx_n = \\mathbf{x}^T \\mathbf{w}$$\n",
    "$$ \\hat{y} = h_{\\mathbf{w}}(\\mathbf{x}) = h(z) = h(\\mathbf{x}^T \\mathbf{w})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a6de1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/perceptron.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48397f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de un perceptrón utilizando la biblioteca Scikit-Learn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "per = Perceptron()\n",
    "per.fit(X_train, y_train)\n",
    "print(\"Training score:\", per.score(X_train, y_train)) # Accuracy de entrenamiento\n",
    "print(\"Test score:\", per.score(X_test, y_test)) # Accuracy de testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Los valores reales:\", y_test)\n",
    "print(\"Predicciones:\", per.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28124039",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec40d6",
   "metadata": {},
   "source": [
    "- Al entrenar una neurona, lo que se intenta determinar son los **pesos** $\\mathbf{w}$ tales que la predicción $h_{\\mathbf{w}}(\\mathbf{x})$ sea lo más cercana al valor esperado.\n",
    "- Considere el Suma de Errores Cuadrático (SSE) entre la salida de la neurona ($\\hat{y}$) y el valor real esperado ($y$).\n",
    "    $$ J(\\mathbf{w}) = \\text{SSE} = \\frac{1}{2} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)}  - y^{(i)} \\right)^2 = \\frac{1}{2} \\sum_{i=1}^{m} \\left( h(\\mathbf{w}^T \\mathbf{x}^{(i)} )  - \\mathbf{y}^{(i)} \\right)^2$$\n",
    "donde $\\mathbf{X}$ es la matriz de datos de entrada (datos de entrenamiento), $\\mathbf{w}$ es el vector de pesos, $\\mathbf{y}$ es el vector de salidas esperadas, $m$ es el total de muestras (datos de entrenamiento).\n",
    "\n",
    "Al necesitarse que los valores de $\\hat{\\mathbf{y}}$ e $\\mathbf{y}$ sean los más cercanos posibles, el SSE puede ser utilizado como médida. Por lo tanto, **debemos ajustar los valores de $\\mathbf{w}$ de tal manera que el SSE sea el *menor* posible**.\n",
    "\n",
    "$$ \\min_{\\mathbf{w}} J(\\mathbf{w}) = \\min_{\\mathbf{w}} \\frac{1}{2} \\sum_{i=1}^{m} \\left( h(\\mathbf{w}^T \\mathbf{x}^{(i)} )  - \\mathbf{y}^{(i)} \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd420d1",
   "metadata": {},
   "source": [
    "- El algoritmo de descenso del gradiente o Gradient Descent, permite calcular el mínimo local de una función\n",
    "- Se define a través de la siguiente regla de actualización para cada elemento $w_j$ perteneciente al vector de pesos $\\mathbf{w}$.\n",
    "\n",
    "$$ w_j := w_j - \\alpha \\nabla_{w_j}J(\\mathbf{w})   $$\n",
    "- Gradient Descent se utiliza para actualizar los pesos de la neurona al momento de entrenar.\n",
    "- Actualiza el valor de cada peso $w_j$ utilizando el gradiente de la función de costo definida anteriormente como base para la actualización (avanzo por el lugar que minimice el error de $J(\\mathbf{w})$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29101d5",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "\n",
    "- Back Propagation es un algoritmo que permite el cálculo de los gradientes requeridas por Gradient Descent utilizando la regla de la cadena.\n",
    "- Estas derivadas se \"propagan hacia atrás\" de manera que el cálculo de una derivada se puede reutilizar para múltiples derivadas que dependan de ella por la regla de la cadena.\n",
    "- Considere la siguiente red neuronal simple:\n",
    "\n",
    "<img src=\"images/nn_simple.png\" width=\"700px\">\n",
    "- Calculando la función de costo para una instancia, se obtiene lo siguiente:\n",
    "\n",
    "$$ J = (\\textbf{a}^{(3)} - \\textbf{y})$$\n",
    "\n",
    "Donde $\\textbf{a}^{(3)}$ se puede definir de la siguiente manera:\n",
    "\n",
    "$$\\textbf{a}^{(3)} = \\textbf{h}^{(3)}\\left(\\textbf{z}^{(3)}\\right) = \\textbf{h}^{(3)}\\left( \\textbf{a}^{(2)} \\cdot \\textbf{w}^{(3)} \\right)$$ \n",
    "\n",
    "donde $\\textbf{h}^{(i)}$ representa la función de activación de la capa $i$\n",
    "- A continuación calcularemos cómo varía la función de costo (error) cuando modifico $\\textbf{w}^{(3)}$. Por regla de la cadena, se tiene\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\textbf{w}^{(3)}} = \\frac{\\partial \\textbf{z}^{(3)}}{\\partial \\textbf{w}^{(3)}} \\cdot \\frac{\\partial \\textbf{a}^{(3)}}{\\partial \\textbf{z}^{(3)}} \\cdot \\frac{\\partial J}{\\partial \\textbf{a}^{(3)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123cb0d3",
   "metadata": {},
   "source": [
    "- Calculando cada derivada\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\frac{\\partial J}{\\partial \\textbf{a}^{(3)}} &= \\frac{\\partial}{\\partial \\textbf{a}^{(3)}} (\\textbf{a}^{(3)} - \\textbf{y})^2 = 2(\\textbf{a}^{(3)} - \\textbf{y}) \\\\\n",
    "    \\frac{\\partial \\textbf{a}^{(3)}}{\\partial \\textbf{z}^{(3)}} &= \\frac{\\partial}{\\partial \\textbf{z}^{(3)}} \\textbf{h}^{(3)}(\\textbf{z}^{(3)}) = \\textbf{h}'^{(3)}(\\textbf{z}^{(3)})\\\\\n",
    "        \\frac{\\partial \\textbf{z}^{(3)}}{\\partial \\textbf{w}^{(3)}} &= \\frac{\\partial}{\\partial \\textbf{w}^{(3)}}(\\textbf{a}^{(2)} \\textbf{w}^{(3)}) = \\textbf{a}^{(2)}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "- Todas las derivadas anteriores son fácilmente calculables, excepto $\\textbf{h}'^{(3)}$, pues depende de la función de activación escogida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb33704",
   "metadata": {},
   "source": [
    "- Veamos qué ocurre cuando calculamos cómo varía la función de costo cuando modificamos $\\textbf{w}^{(2)}$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\frac{\\partial J }{\\partial \\textbf{w}^{(2)}} &= \\frac{\\partial \\textbf{z}^{(2)}}{\\partial \\textbf{w}^{(2)}} \\cdot \\frac{\\partial \\textbf{a}^{(2)}}{\\partial \\textbf{z}^{(2)}} \\cdot \\frac{\\partial \\textbf{z}^{(3)}}{\\partial \\textbf{a}^{(2)}} \\cdot \\boxed{\\frac{\\partial a\\textbf{a}^{(3)}}{\\partial \\textbf{z}^{(3)}} \\cdot \\frac{\\partial J}{\\partial \\textbf{a}^{(3)}}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "donde las últimas dos derivadas fueron también calculadas para $\\frac{\\partial J}{\\partial \\textbf{w}^{(3)}}$. Eso quiere decir que la derivadas ya calculadas pueden ser reutilizadas o **propagadas hacia atrás** en el cálculo de Gradient Descent para otros pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb51c9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes multicapas y Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a651bcce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es posible conectar múltiples neuronas entre sí tomando la salida de una neurona como la entrada para una o múltiples neuronas adicionales.\n",
    "- Las conexiones que se producen entre neuronas conforman lo que se llama como **red neuronal**.\n",
    "- Dependiendo de cuales sean las conexiones entre distintas neuronas, se habla de la **arquitectura** de la red neuronal.\n",
    "- Las redes neuronales se dividen en **capas**, las cuales se dividen en las siguientes categorías:\n",
    "    - Capas de entrada (input layers)\n",
    "    - Capas de salida (output layers)\n",
    "    - Capas ocultas/intermedias (hidden layers)\n",
    "- Las redes multicapas están compuestas por una capa de entrada, una o varias capas ocultas y una capa de salida\n",
    "- Cuando una red multicapa contiene muchas capas ocultas, hablamos de una **Red Neuronal Profunda** (Deep Neural Network - DNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b1843",
   "metadata": {},
   "source": [
    "<img src=\"images/dnn.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8178fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Al igual que en el perceptron, deseamos calcular los pesos $w$ de todas las neuronas tales que la salida esté lo más cercana a los valores esperados. \n",
    "- ¿Cómo podemos calcular tantos pesos a la vez? \n",
    "    - **R:** Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e930b",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eaf8e8",
   "metadata": {},
   "source": [
    "- Back propagation es el procedimiento que se utiliza para calcular los valores de las derivadas parciales $\\nabla_{w_j}J(\\mathbf{w})$ de la función de costo sobre cada peso de la red.\n",
    "- Se basa en la regla de la cadena para propagar los gradientes de capas posteriores para calcular los gradientes de capas anteriores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd309613",
   "metadata": {},
   "source": [
    "# Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3975d0",
   "metadata": {},
   "source": [
    "- Las funciones de activación permiten agregar no linealidades a una neurona.\n",
    "- Existen muchas funciones de activación. Algunas enfocadas en **regresión** (predicción de valores numéricos) y otras en **clasificación** (predicción de clases).\n",
    "- Ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd246ad4",
   "metadata": {},
   "source": [
    "### Linear\n",
    "$$\\text{Linear}(x)=x$$\n",
    "\n",
    "### Sigmoide\n",
    "- Útil en problemas de clasificación (entrega valores entre 0 y 1 que se interpretan como probabilidad)\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "- Utiliza en capas intermedias por su fácil cálculo de derivadas y agregar no linealidades de manera sencilla.\n",
    "$$\\text{ReLU}(x) = \\max(0, x) $$\n",
    "\n",
    "### Leaky ReLU\n",
    "- Uso similar a ReLU; evita que neuronas tenga activación cero.\n",
    "$$\\text{LeakyReLU}(x) = \\begin{cases}\n",
    "x, & \\text{si } x > 0 \\\\\n",
    "\\alpha x, & \\text{si } x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Donde $\\alpha$ es un valor pequeño y positivo.\n",
    "\n",
    "### Función Tangente Hiperbólica (Tanh)\n",
    "- Se utiliza para introducir no linealidad en capas intermedias\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "### Función Softmax\n",
    "- Se utiliza en problemas de clasificación multi clase (equivalente a sigmoide pero para varias clases).\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c465e",
   "metadata": {},
   "source": [
    "# Google Colaboratory\n",
    "<img src=\"images/colab_favicon_256px.png\" width=\"100px\">\n",
    "\n",
    "- Proyecto basado en Jupyter Notebook (sistema ampliamente usado en Data Science)\n",
    "- Permite escribir y ejecutar código Python en el navegador \n",
    "    - Sin configuración requerida\n",
    "    - Acceso gratuito a GPU (Graphic Process Unit)\n",
    "    - Facilidad para compartir (en la nube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0301c0b",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/colab_note.png\" width=\"900px\" >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2468f774",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bibliotecas de Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0334dc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Actualmente existe una amplia gama de bibliotecas para Deep Learning. \n",
    "- Por su facilidad de uso y popularidad, utilizaremos la API de **Keras**, la cual viene incluida dentro de la biblioteca **Tensorflow**.\n",
    "- Para poder utilizar Keras, es necesario tener previamente instalado Tensorflow (```pip install tensorflow```)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61a9ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/bibliotecas.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdde7a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creando redes neuronales en Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1b71c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para crear una red neuronal, debemos\n",
    "- Definir la arquitectura de la red (modelo y capas)\n",
    "- Compilar el modelo (selección de optimizador, función de pérdida y métricas)\n",
    "- Entrenar modelo\n",
    "- Usar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c60ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo usando fashion mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Cost\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"] # 10 clases\n",
    "\n",
    "X_train_full = X_train_full/255.0 # Escalamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b68ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda muestra algunas imágenes del dataset como ejemplo\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(6,6,i+1)\n",
    "    ax.imshow(X_train_full[i], cmap='gist_yarg')\n",
    "    ax.set_title(class_names[y_train_full[i]])\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá armaremos nuestra red neuronal (la arquitectura)\n",
    "# Recuerda que en este punto se debe escoger tanto el modelo, la cantidad de capas, el tipo de capas,\n",
    "# cantidad de neuronas, etc.\n",
    "# Armado de nuestra red neuronal\n",
    "model = tf.keras.models.Sequential() # Modelo secuencial\n",
    "model.add(tf.keras.layers.Flatten(input_shape=[28, 28])) # Capa de entrada, tiene dimensiones 28x28\n",
    "model.add(tf.keras.layers.Dense(300, activation='relu')) # Primera capa oculta, salida 300\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu')) # Segunda capa oculta, salida 100\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax')) # Capa de salida, salida 10 (número de clases)\n",
    "# Importante \n",
    "# -Para un modelo de clasificación multiclase debes tener tantas neuronas en la capa de salida como clases tenga\n",
    "# tu dataset\n",
    "# - La función de activación DEBE ser softmax de la capa de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red creada\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9366c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el proceso de compilación, la función de perdida DEBE ser sparse_categorical_crossentropy\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Ejemplos de métricas\n",
    "# metrics=[tf.keras.metrics.Accuracy(), tf.keras.metrics.FalsePositives()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Utilización de datos de validación y entrenamiento del modelo\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, \n",
    "                                                  stratify=y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fe7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test/255.0 # nuevamente, escalamiento de set de datos de entrenamiento\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(\"Loss:\", loss) # Función de pérdida\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#dato = 56\n",
    "#dato = 74\n",
    "dato=12\n",
    "prediction = model.predict(X_test)\n",
    "print(prediction[12])\n",
    "classes_x=np.argmax(prediction,axis=1)\n",
    "print(\"Predicción:\", class_names[classes_x[dato]])\n",
    "print(\"Clase Real:\", class_names[y_test[dato]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cd8eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularización en NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56121e67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- La gran cantidad de parámetros que posee una red neuronal, si bien, entrega una gran flexbilidad de ajuste a una enorme cantidad de datos distintos, esto también las hace propensas al overfitting (sobre ajuste).\n",
    "- Para evitar el overfitting, al igual en otros métodos, necesitamos utilizar técnicas de **regularización**.\n",
    "- Algunas de las técnicas más populares son:\n",
    "    - Regularización $l_1$ y $l_2$\n",
    "    - Dropout (dilución)\n",
    "    - Early stop (detención anticipada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0e98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/nba.csv', index_col = 'Name')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Botar faltantes\n",
    "df.dropna(inplace = True)\n",
    "# Guardar X datos\n",
    "X = df.drop(columns = 'TARGET_5Yrs')\n",
    "# Codificar nuestro objetivo\n",
    "y = df['TARGET_5Yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División entrenamiento, validación, prueba\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a504ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamiento\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Input shape\n",
    "input_shape = X_train.shape[1]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo sin regularización\n",
    "model = Sequential()\n",
    "# Primera capa oculta\n",
    "model.add(Dense(19,\n",
    "                input_dim = input_shape, \n",
    "                activation = 'relu')) \n",
    "model.add(Dense(10, \n",
    "                activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'bce', optimizer = 'sgd', metrics=['accuracy']) # bce: binary cross entropy\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_val, y_val), \n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07410f2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizar la perdida\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd5936",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularización $l_1$ y $l_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581183c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Las regularización $l_1$ y $l_2$ agregan factores a la función de costo de una red neuronal para impedir que los pesos de esta tomen cualquier valor (multiplicadores de Lagrange).\n",
    "- Las regularizaciones de tipo $l_1$ y $l_2$ se definen de la siguiente manera sobre la función de costo:\n",
    "    - Regularización $l_1$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    J = \\frac{1}{m}\\sum_{i=1}^{m} \\left( \\hat{\\textbf{y}}^{(i)} - \\textbf{y}^{(i)} \\right)^{2}  + \\boxed{\\alpha \\frac{1}{n} \\sum_{j=1}^{n} |\\textbf{w}_{i} |}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "    - Regularización $l_2$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    J = \\frac{1}{m}\\sum_{i=1}^{m} \\left( \\hat{\\textbf{y}}^{(i)} - \\textbf{y}^{(i)} \\right)^{2}  + \\boxed{\\alpha \\frac{1}{2n} \\sum_{j=1}^{n} \\textbf{w}_{i}^2}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ecb0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "# Con Regularización l2\n",
    "# Modelo secuencial\n",
    "model = Sequential()\n",
    "# Primera capa oculta\n",
    "model.add(Dense(19, # Cuántas neuronas tienes en tu primera capa oculta\n",
    "                input_dim = input_shape, # ¿Cuál es la forma de tus características de entrada (número de columnas)?\n",
    "                activation = 'relu',\n",
    "                kernel_regularizer=l2(0.01))) # ¿Qué función de activación estás usando?\n",
    "model.add(Dense(10, \n",
    "                activation = 'relu',\n",
    "                kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=['accuracy'])\n",
    "history_l2 = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_val, y_val), \n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cd982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_l2.history['loss'], label='Train loss')\n",
    "plt.plot(history_l2.history['val_loss'], label='Val Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1eabdd",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e087b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Un algoritmo sencillo que ha mostrado ser muy efectivo al entrenar redes neuronales\n",
    "- En cada iteración de entrenamiento, cada neurona (excepto las neuronas de salida) tiene una probabilidad $p$ de ser **excluída** (ignorada).\n",
    "- La probabilidad $p$ es llamada **tasa de dropout**. Por lo general esta tasa se escoge como:\n",
    "    - Entre 40%-50% para redes convolucionales\n",
    "    - Entre 20%-30% para redes recurrentes\n",
    "    - Para otras redes, se puede escoger valores entre el 10% hasta el 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da2a66",
   "metadata": {},
   "source": [
    "<img src=\"images/dropout.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c26766",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Con Dropout\n",
    "# Modelo secuencial\n",
    "model = Sequential()\n",
    "# Primera capa oculta\n",
    "model.add(Dense(19, # Cuántas neuronas tienes en tu primera capa oculta\n",
    "                input_dim = input_shape, # ¿Cuál es la forma de tus características de entrada (número de columnas)?\n",
    "                activation = 'relu')) # ¿Qué función de activación estás usando?\n",
    "# A continuación agregamos nuestra capa de dropout\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(10, \n",
    "                activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=['accuracy'])\n",
    "history_dropout = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_val, y_val), \n",
    "                    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daab4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_dropout.history['loss'], label='Train loss')\n",
    "plt.plot(history_dropout.history['val_loss'], label='Val Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86cee69",
   "metadata": {},
   "source": [
    "# Early stop (detención temprana)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140f9322",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Para algoritmos iterativos que tienden al overfitting, el early stopping propone dejar de entrenar tan pronto se alcanza el mínimo error. \n",
    "- En el ejemplo se ve que el error de validación aumenta en la medida que aumentan los epochs. Early stop mantendrá aquel modelo que entrega el menor error de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9336cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Con detención anticipada\n",
    "# Modelo secuencial\n",
    "model = Sequential()\n",
    "# Primera capa oculta\n",
    "model.add(Dense(19, # Cuántas neuronas tienes en tu primera capa oculta\n",
    "                input_dim = input_shape, # ¿Cuál es la forma de tus características de entrada (número de columnas)?\n",
    "                activation = 'relu')) # ¿Qué función de activación estás usando?\n",
    "model.add(Dense(10, \n",
    "                activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'bce', optimizer = 'adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience = 5) # Número de iteraciones sin mejora antes de parar el entrenamiento\n",
    "history_earlystop = model.fit(X_train, y_train,\n",
    "                    validation_data = (X_test, y_test), \n",
    "                    epochs=100,\n",
    "                    callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_earlystop.history['loss'], label='Train loss')\n",
    "plt.plot(history_earlystop.history['val_loss'], label='Validation Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe47f38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Redes Neuronales Recurrentes (RNN-Recurrent Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200b4e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hasta el momento hemos trabajado con redes FeedForward, donde las activaciones fluyen en una dirección: desde la capa de entrada a la capa de salida.\n",
    "- Las redes neuronales recurrentes son similares a las redes FeedForward, salvo que también existen conexiones en el sentido inverso.\n",
    "- Las redes neuronales recurrentes son utilizadas generalmente para procesar secuencias de datos temporalmente relacionados, pues toman en cuenta lo que ocurrió previamente\n",
    "- Las unidades de procesamiento que normalmente se utilizan son\n",
    "    - Long Short Term Memory (LSTM)\n",
    "    - Gated Recurrent Unit (GRU)\n",
    "- Las redes Neuronales Recurrentes son muy utilizadas para predicciones de señales y en aplicaciones de NLP (Natural Language Processing)\n",
    "\n",
    "<img src=\"images/rnn.png\" width=\"700px\">\n",
    "Fuente: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bb79e0",
   "metadata": {},
   "source": [
    "## Tipos de arquitecturas de Redes Neuronales Recurrentes\n",
    "\n",
    "- **Many to One**: Múltiples entradas son usadas para la obtención de una salida\n",
    "\n",
    "<img src=\"images/many-to-one.jpeg\" width=\"400px\">\n",
    "\n",
    "- **One to Many**: Una única entrada produce múltiples salidas\n",
    "\n",
    "<img src=\"images/one-to-many.png\" width=\"400px\">\n",
    "\n",
    "- **Many to many**: Múltiples entradas producen múltiples salidas\n",
    "\n",
    "<img src=\"images/many-to-many.png\" width=\"500px\">\n",
    "\n",
    "Fuente: https://medium.com/analytics-vidhya/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8170716",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ejemplo de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fc1af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset de manchas solares disponible en https://github.com/jbrownlee/Datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "df = pd.read_csv('data/sunspots.csv', \n",
    "                 index_col = 'Date')\n",
    "df = df.drop(columns=['id'])\n",
    "df.rename(columns={'Monthly Mean Total Sunspot Number': 'sunspots'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3765cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "df['sunspots'].plot()\n",
    "plt.ylabel('Manchas solares');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6494285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division train/test\n",
    "train_values = np.array(df.loc[:'2016-12-31', 'sunspots'])\n",
    "test_values = np.array(df.loc['2017-01-31':, 'sunspots'])\n",
    "\n",
    "# Escalamiento\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "train = scaler.fit_transform(train_values.reshape(-1, 1))\n",
    "test = scaler.transform(test_values.reshape(-1, 1))\n",
    "\n",
    "# Conversión a datos con tres columnas\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "prev_months = 1\n",
    "generator = timeseries_dataset_from_array(\n",
    "    train,\n",
    "    train,\n",
    "    sequence_length=prev_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab68428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo secuencial\n",
    "model = Sequential()\n",
    "# Capa LSTM\n",
    "model.add(LSTM(units = 50, \n",
    "               return_sequences = False # True si la siguiente capa será una capa recurrente\n",
    "               ))\n",
    "# Capa de salida\n",
    "model.add(Dense(units = 1, activation = 'relu'))\n",
    "\n",
    "# Compilado\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "\n",
    "history_lstm = model.fit(generator, epochs=50) # el generador creará los datos en cada iteración\n",
    "\n",
    "plt.plot(history_lstm.history['loss'], label='Train loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c40589",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c190a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "full = np.concatenate((train, test))\n",
    "test_days = pd.DataFrame(full)[len(full) - len(test) - prev_months:].values\n",
    "generator_test = timeseries_dataset_from_array(\n",
    "    test_days,\n",
    "    test_days,\n",
    "    sequence_length=prev_months)\n",
    "\n",
    "# Predicciones\n",
    "preds = model.predict(generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "plt.plot(scaler.inverse_transform(test_days), label='Actual sunspots')\n",
    "plt.plot(scaler.inverse_transform(preds), label='Predicted sunspots')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca9cc2",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales (CNN-Convolutional Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f50d55d",
   "metadata": {},
   "source": [
    "- Así como las redes neuronales recurrentes permiten procesar datos temporalmente relacionados, las redes neuronales convolucionales permiten procesar datos espacialmente relacionados.\n",
    "- Este tipo de redes son utilizadas generalmente para procesar imágenes, donde los pixeles están relacionados espacialmente entre sí.\n",
    "- En las redes convolucionales, generalmente se utilizan **capas de pooling**. Estas capas se utilizan para reducción de dimensionalidad, disminuyendo la redundancia de datos existente en imágenes (pixeles cercanos generalmente están altamente correlacionados)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d957a",
   "metadata": {},
   "source": [
    "## Convolución\n",
    "\n",
    "- Sean dos funciones $f()$ y $g()$, se define la operación de convolución entre ellas de la siguiente manera\n",
    "\n",
    "\\begin{equation}\n",
    "    (f * g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) \\cdot g(t-\\tau)d\\tau\n",
    "\\end{equation}\n",
    "\n",
    "De esta operación se obtiene una nueva función resultante de operar sobre la función $f()$ con la función $g()$ mientras esta se traslada en el tiempo.\n",
    "- La convolución es una operación muy utilizada en el filtrado de señales donde un filtro $g()$ se aplica sobre una señal $f()$\n",
    "\n",
    "<img src=\"images/convolution1d_animation.gif\" width=\"500px\">\n",
    "Imagen Licencia CC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93503ce",
   "metadata": {},
   "source": [
    "- Las redes convolucionales son redes que contienen capas de procesamiento convolucional.\n",
    "- Por lo general este tipo de redes se utiliza para el procesamiento de imágenes, donde se tiene un conjunto de filtros convolucionales bi-dimensionales aplicado sobre una imagen.\n",
    "<img src=\"images/convolution_animation.gif\" width=\"500px\">\n",
    "Fuente: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
    "\n",
    "Ejemplos de formas de aplicar convolución: https://hannibunny.github.io/mlbook/neuralnetworks/convolutionDemos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d150ce",
   "metadata": {},
   "source": [
    "## Capas de Pooling\n",
    "\n",
    "- Las capas de Pooling permiten reducir la dimensionalidad y la varianza espacial de las imágenes en la medida que avanzan por las capas de la red.\n",
    "- Esto permite reconocer patrones más generales en la imagen.\n",
    "\n",
    "### Tipos de Pooling\n",
    "- Max Pooling: Se obtiene el máximo de cada zona\n",
    "\n",
    "<img src=\"images/max-pooling.png\" width=\"500px\">\n",
    "\n",
    "- Average Pooling: Se obtiene el promedio de cada zozna\n",
    "\n",
    "<img src=\"images/avg-pooling.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ea104",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Ejemplo, fuente https://www.tensorflow.org/tutorials/images/cnn\n",
    "# Bibliotecas\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e75265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'Juan', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "print(type(train_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdb937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de modelo\n",
    "model = models.Sequential()\n",
    "# Capas encargadas de obtener información de la imagen\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# Capas para la clasificación en base a la información obtenida en \n",
    "# capas anteriores\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be900643",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history_cnn = model.fit(train_images, train_labels, epochs=30), \n",
    "                    #validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(history_cnn.history['accuracy'], label='train_accuracy')\n",
    "#plt.plot(history_cnn.history['val_accuracy'], label = 'val_accuracy')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.ylim([0.5, 1])\n",
    "#plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 60\n",
    "lala = model.predict(test_images[index].reshape(-1,32,32,3))\n",
    "import numpy as np\n",
    "print(class_names[test_labels[index][0]])\n",
    "print(class_names[np.argmax(lala)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a390f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(False)\n",
    "plt.imshow(test_images[index])\n",
    "plt.xlabel(class_names[test_labels[index][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a3261",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "#codigo extra, para que imagenes de matplotlib\n",
    "#estén centradas en las diapositivas, ejecutar antes de lanzar los ejemplos."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
